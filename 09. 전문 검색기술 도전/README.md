# 9장. 전문 검색기술 도전

**대규모 데이터 처리의 노하우**

## **강의 24: 전문 검색기술의 응용범위**

### **하테나의 데이터로 검색엔진 만들기**

- 하테나 다이어리를 대상으로 전문 검색엔진 구축
- 기본 구조: **Dictionary + Postings** 로 구성된 역 인덱스
- 기존 RDB 기반 검색은 속도 한계 → 인덱싱 방식으로 전환

### **포함하는 블로그 기능**

- 키워드에 해당하는 블로그를 자동으로 식별하는 기능
- 기존 RDB 방식: 해당 키워드로 LIKE 검색 → 레코드 많으면 성능 급락하는 문제
- 검색엔진 도입 후 빠르고 정확한 결과 제공

### **검색기술의 응용**

- 사용자 키워드 입력에 반응하여 즉시 검색결과 제공 가능
- 자체 개발한 검색엔진으로 빠른 처리 가능

### **하테나 북마크의 전문 검색**

- 개별 북마크보다 **개인 북마크 데이터** 검색이 더 중요한 특징이 있었다.
- 사용자별 인덱스를 따로 구성하는 방식을 사용해서 최적화
    - 개인별 데이터가 크고 검색 패턴이 다르기 때문
    - 개인 인덱스는 작은 데이터를 빠르게 처리하는 데 유리
- 병렬 구성 필요: 데이터 규모가 사용자마다 많이 달라서

## **강의 25: 검색 시스템의 아키텍처**

### **검색엔진 구성 단계**

1. **크롤링** – 대상 문서 가져오기
2. **저장** – 문서 저장소 구성
3. **인덱싱** – 빠른 검색을 위한 역 인덱스 구축
4. **검색** – 검색쿼리를 포함하는 문서 찾기
5. **스코어링** – 문서 중요도 계산 → 검색결과를 어떤 순서로 표시해줄 것인가
6. **결과 표시** – 사용자에게 결과 반환

### **전문 검색엔진의 종류와 각각의 특징(통합 정리)**

- 전문 검색엔진은 **문서를 어떻게 검색하는가**에 따라 크게 **세 가지 방식**으로 나눌 수 있다.

### **1) grep형 검색엔진 (문서 전체 직접 스캔 방식)**

- 문서를 **처음부터 끝까지 직접 읽으면서** 패턴을 찾는 방식
- UNIX의 grep과 유사한 처리 방식
- 구조가 매우 단순하고 구현이 쉬움
- But, 문서가 많아질수록 **검색 시간 = O(m + n) 이나 O(mn)**  으로 폭증
- 매 요청마다 전체를 다시 읽기 때문에 **캐싱 최적화 어려움**

### **2) Suffix형 검색엔진 (Suffix 구조 기반 고속 검색)**

- 문서 전체를 문자열로 보고, 이를 부분 문자열 단위로 검색 가능하게 만들어주는 구조.
- **Trie, Suffix Array, Suffix Tree 등의 구조 사용**
- **부분 문자열 검색**에 매우 강함 (예: “abc”가 포함된 모든 문서 탐색)
- 인덱스 생성 후 검색 속도 매우 빠름
- 인덱스가 복잡하고 메모리 사용량 크다. 또, 구현 난이도가 높다.

### **역 인덱스형 검색엔진 (Inverted Index 기반)**

- 오늘날 **Google·Elasticsearch·Lucene·Solr 등 거의 모든 검색엔진의 표준 구조**
- 문서를 읽어서 **term(단어)** → **문서ID 리스트(Postings)** 형태로 매핑하는 인덱스를 생성
- 검색은 term을 기반으로 postings 리스트에서 **교집합(intersection)** 계산
- 스코어링(TF-IDF), PageRank 등 다양한 랭킹 알고리즘 적용 가능
- 색인 구조가 단어 단위라 **의미 기반 검색, 형태소 분석, n-gram** 등 확장 용이
- But, 인덱스 생성 비용이 크다. (초기 색인 시간이 필요)
- 실시간 업데이트 처리에는 별도 구조 필요 (Log-Structured Merge Tree 등)

## **강의 26: 검색엔진의 내부구조 (상세)**

### **역 인덱스 = Dictionary + Postings**

- Dictionary: term 목록
- Postings: term이 포함된 문서ID 목록
- 검색은 term → postings 리스트를 읽어 “교집합(intersection)”으로 후보를 결정

<img width="691" height="359" alt="image" src="https://github.com/user-attachments/assets/14cd993e-1f30-4809-ab19-7b3ab6b75379" />

### **Dictionary 만드는 방법**

### **방법 1) 사전 + AC법**

- 사전 기반 검색 → 사전에 정의된 단어만 검색 가능
- 텍스트에서 사전에 있는 단어 찾기
- 일본어/한국어처럼 형태소 필요한 경우에도 유효
- MeCab, JUMAN, Chasen 등 형태소 분석기 활용 가능
- 사전은 커스터마이징 가능

### **방법 2) 형태소 분석 사용**

- 문장을 품사 단위로 나눔
- 문맥 기반으로 단어를 판단할 수 있음
- 단점: 구현 복잡, 사전 관리 비용 큼
- 장점: 정확도 높음

### **방법 3) n-gram 사용**

- 언어 독립적 방식
- 텍스트를 n문자씩 잘라 term으로 등록
- 예: “hatena” → ha, at, te, en, na (2-gram)
- 장점: 사전 없이도 만들 수 있음
- 단점: term 수 증가로 필터링 필요

<img width="690" height="336" alt="image" src="https://github.com/user-attachments/assets/102e2b1e-1a6a-4785-a4cc-c82efeca70f4" />

### **n-gram 분할 문제와 필터링**

- 쿼리도 n-gram으로 분할해서 검색하게 되는데 n-gram으로는 잘못된 검색을 수행하는 문제가 있다.
- 예를 들어, “부산역” 이라는 문자를 포함한 문서가 있을 때, 이를 2-gram 으로 term등록을 해둔다면, “**부산**물은 안전하게 처리해야 한다.” 라는 문자도 검색 후보이다.
- 검색결과를 다시 확인하는 **후처리 필터링** 필요 (이 문자가 문서에 정말로 포함되어 있는지 확인)

### **재현율(Recall)·적합률(Precision)**

- **재현율 = C / A**
    - 전체 관련 문서 중 검색된 문서 비율
- **적합률 = C / B**
    - 검색된 문서 중 실제 관련 문서 비율
- 둘은 상충 관계 → 목적에 따라 균형 조절 필요
    - 검색 폭 넓히면 재현율↑ 적합률↓
- 정확한 것만 검색하면 재현율↓ 적합률↑

<img width="693" height="257" alt="image" src="https://github.com/user-attachments/assets/759cd45d-3e15-4c5e-878c-0fc69d64508d" />

### **Postings 작성법**

### 방법 1) **Full Inverted Index (출현 위치 저장)**

- term → (문서ID + 위치)
- 스니펫 생성, 랭킹 등에 유리

### 방법 2) **문서ID만 저장**

- term → 문서ID 리스트
- key-value 스토리지 사용에 적합
- 하테나 북마크 검색에서 사용

### **VB Code / 압축 기법**

- 문서 ID 리스트를 더 작게 표현하는 기법
- 대량 데이터 환경에서 중요

### **스코어링**

- 문서 중요도를 계산하는 핵심
- Google의 대표 스코어링: PageRank
- 텍스트 기반 점수(TF/IDF 등)와 조합하여 사용 (Term Frequency / Inverse Document Frequency)

## 9장. 전문 검색기술 도전 **요약**

- 검색엔진의 핵심은 빠르게 검색하기 위해 미리 인덱스를 만드는 것
- 역 인덱스 방식이 표준이며 Dictionary + Postings 구조로 구성된다.
- term 생성은 사전·형태소·n-gram 세 방식이 존재
- 대규모 서비스는 사용자별 인덱스/서비스별 인덱스로 확장
- 검색 품질 평가는 재현율과 적합률로 판단
- 스코어링은 단순히 매칭이 아닌 문서의 중요도를 반영한다.
