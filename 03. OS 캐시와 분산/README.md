# 3장. OS 캐시와 분산

**대규모 데이터를 효율적으로 처리하는 원리**

## **강의 8: OS의 캐시 구조**

### **OS 캐시가 존재하는 이유**

- 디스크 I/O는 메모리 접근보다 10⁵~10⁶배 느리다.
- OS는 디스크 접근을 최소화하고, 메모리를 활용해 디스크 데이터를 빠르게 공유하기 위해 **페이지 캐시(page cache)** 를 구성한다.
- 프로그램은 실제 디스크가 아닌 캐시(메모리)로부터 대부분의 데이터를 읽어온다.

### **페이지 캐시의 기본 개념**

- 디스크에서 읽은 데이터 → 메모리의 페이지 캐시에 저장
- 같은 파일·데이터를 요청하는 다른 프로세스는 디스크를 다시 읽지 않고 페이지 캐시에서 읽는다.
- OS는 파일캐시, 버퍼캐시 등을 통합된 페이지 캐시 구조로 운영한다.
    
    (Linux는 page cache 내부에 buffer cache가 들어가는 형태)
    

### **가상 메모리 구조와 페이지**

- 프로세스가 접근하는 주소는 실제 메모리가 아닌 **가상 주소**이다.
- OS는 페이지 단위(보통 4KB)로 물리 메모리와 매핑한다.
- 프로세스는 **연속된 주소 공간**을 사용하지만 실제 물리 메모리는 조각나 있어도 상관 없다.
- OS가 페이지 테이블을 통해 가상주소 → 물리주소 매핑을 수행한다.

### **가상 메모리 구조**

<img width="695" height="468" alt="image" src="https://github.com/user-attachments/assets/6dcadcf7-eb4a-4920-b852-79c9c89ab574" />

- 프로세스가 메모리를 요청 → OS는 빈 페이지 프레임을 찾고 매핑한다.
- 실제 메모리에는 0x00002123 같은 주소에 위치히지만, 프로세스는 전혀 다른 주소를 받는다.
- OS는 필요한 페이지가 없는 경우 디스크에서 페이지 단위로 읽어 메모리에 로드한다.

### **캐시와 디스크 읽기 과정**

<img width="720" height="514" alt="image" src="https://github.com/user-attachments/assets/9cea780b-f2d1-4433-852c-126791f690fb" />

- 프로세스가 파일을 읽는다. → OS는 디스크에서 ‘4KB 블록’ 단위로 읽어 페이지 캐시에 저장한다.
- 이후 동일 데이터를 읽는 요청은 디스크가 아닌 캐시에서 즉시 처리된다.
- 페이지 캐시 덕분에 디스크 I/O 발생을 획기적으로 줄일 수 있다.

### **OS 캐시가 없다면**

- 매 요청마다 디스크로 접근 → 엄청난 iowait 증가
- 프로그램 처리 속도 저하, 스레드 대기 증가, 병렬 처리 성능 악화
- 특히 DB 서버는 캐시가 없으면 사실상 동작 불가능 수준이다.
- 따라서, **메모리를 늘려서 I/O 부하 줄이기**는 유효한 전략이다.

## **강의 9: I/O 부하를 줄이는 방법**

### **메모리를 늘리면 I/O가 줄어드는 이유**

- 여유 메모리가 많으면 더 많은 페이지 캐시를 유지할 수 있다.
- 캐시된 내용은 디스크를 다시 읽을 필요가 없으므로 자연스럽게 iowait이 감소한다.
- sar 측정 결과
    - 4GB 메모리일 때 iowait 약 20%
    - 8GB로 늘리자 iowait 거의 사라짐

### **캐시는 남은 메모리 전체를 사용한다.**

- Linux는 free 메모리가 있으면 자동으로 page cache로 사용한다.
- sar에서 kbmemfree는 줄고, kbcached는 증가하는 형태로 확인할 수 있다.
- 물리 메모리가 크면 클수록 캐시가 커지고 I/O 감소 폭도 크다.

### **단순히 서버 대수만 늘려서는 확장성을 확보할 수 없다.**

<img width="678" height="270" alt="image" src="https://github.com/user-attachments/assets/5d9ed554-a5a6-4892-bf75-b138ae6dacff" />

- 동일한 데이터를 가진 서버를 여러 대 둬도 캐시 데이터가 서로 공유되지 않아 효과가 미비하다.
- 서버 수만 늘려서는 확장성을 확보할 수 없고, 데이터를 분산해서 저장해야 한다.

### **페이지 캐시 적중률의 중요성**

- 캐시 적중률이 낮다면 디스크 I/O가 폭증하고 iowait이 증가한다.
- 특히 RDBMS 같은 시스템은 캐시 의존도가 매우 커서, 캐시 미스가 많으면 전체 서비스가 느려진다.

## **강의 10: 국소성을 살리는 분산**

### **국소성(Locality) 기반 분산의 필요성**

- 전체 데이터를 캐시할 수 없는 경우 잘 접근되는 데이터끼리 묶어주는 전략이 매우 중요하다.
- 단순히 서버를 늘리는 것이 아니라, 데이터 접근 패턴에 따라 서버를 분리해야 캐시 효율이 극대화된다.

### **테이블 단위 분할**

<img width="704" height="204" alt="image" src="https://github.com/user-attachments/assets/9d4e66e5-64a4-4c4a-8808-a0c0c8f09812" />

- 테이블 자체를 여러 서버로 분할하는 방법
- 예: entry, bookmark, tag, keyword 테이블을 각기 다른 서버에 배치

### 테이블 데이터 분할

<img width="685" height="351" alt="image" src="https://github.com/user-attachments/assets/68eb0a10-d32e-49fa-a681-9a0e0629c582" />

- 테이블의 데이터를 분할하는 방법
- 테이블의 특정 값을 기준으로 다른 서버에 저장
- 예: id=a~c는 서버1, id=d~f는 서버2

### **요청 패턴 기반 분할 (섬 분할)**

<img width="694" height="578" alt="image" src="https://github.com/user-attachments/assets/a4a5f0e1-a60e-47f2-bff4-990c49d570e5" />

- 일반 페이지 요청과 heavy API 요청을 분리하는 등 특정 패턴에 의한 분할 방법
- 각 섬은 서로 다른 캐시를 유지하며 각자의 데이터 접근 패턴에 최적화
- 검색 엔진 봇, 스크래퍼, 이미지 API 등은 별도의 섬에서 처리

## **강의 11: 페이지 캐시를 고려한 운영의 기본 규칙**

### **운영자가 반드시 알아야 할 규칙**

- OS 캐시를 이해하지 못한 채 서버를 운영하면 문제가 발생할 수 있다.
- 다음 상황에서 캐시 고려가 필수:
    - DB 서버 운영
    - 대규모 파일 서비스
    - 로그/검색 시스템
    - API 트래픽 증가 상황

### **메모리 증설은 가장 확실한 성능 개선책**

- 메모리를 늘려 캐시 용량을 확보하면 I/O 부하를 직접적으로 줄일 수 있다.
- 8GB → 16GB만 되어도 체감 효과가 크다.
- 메모리 증설로 대응할 수 없다면 분산을 고려한다.

### **캐시가 사라지면 어떻게 되는가?**

- 캐시가 사라지는 순간 모든 요청이 디스크를 액세스하게 되어 성능이 급격히 저하된다.
- 대량의 iowait 발생 → 서비스 응답 지연 → 스레드 증가 → 장애 발생 가능
- 따라서, IO작업이 많은 서버를 재부팅하고 나면 운영단계로 넘어가기 전에 캐시를 미리 로드하는 등 전략이 필요하다.

## 3장. OS 캐시와 분산 **요약**

- 디스크 I/O는 매우 느리므로 OS는 **페이지 캐시**를 통해 이를 해결한다.
- 가상 메모리 + 페이지 구조가 캐시의 기반이다.
- 메모리를 늘리면 캐시 용량이 증가하여 I/O 부하가 감소한다.
- 단순히 서버 수를 늘리는 것은 확장성이 없다. → 국소성 기반 분산이 필요하다.
- 파티셔닝, 요청 패턴 분산으로 캐시 효율을 극대화한다.
- 운영자는 항상 **캐시가 어떻게 잡히는가**를 고려해야 고성능 시스템을 만들 수 있다.
